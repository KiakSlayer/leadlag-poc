# Project Structure Overview

This document summarizes the purpose of each module in the repository and highlights the most important capabilities that are implemented in code.

## Main Cross-Asset Workflow

| File | Purpose |
| ---- | ------- |
| [`main_crossasset_poc.py`](main_crossasset_poc.py) | Command-line entry point that orchestrates the end-to-end pipeline: fetches data, aligns timestamps, analyzes correlations, runs the strategy, backtests, and optionally saves reports. |
| [`data_fetcher.py`](data_fetcher.py) | Fetches OHLCV data for crypto pairs from Binance and equity indices from Yahoo Finance, then normalizes and aligns the timestamps across assets. |
| [`correlation_analyzer.py`](correlation_analyzer.py) | Computes correlations, detects lead-lag relationships through cross-correlation, and ranks asset pairs for subsequent modeling. |
| [`crossasset_leadlag_model.py`](crossasset_leadlag_model.py) | Implements the rolling window Z-score mean-reversion model that produces trading signals for leader/lagger pairs. |
| [`backtester.py`](backtester.py) | Simulates trades generated by the strategy, builds an equity curve, and calculates performance metrics such as Sharpe, Sortino, and maximum drawdown. |
| [`visualizer.py`](visualizer.py) | Creates multi-panel reports that combine price series, z-scores, signals, equity curves, and performance statistics. |

## Legacy Kafka Streaming Components

| File | Purpose |
| ---- | ------- |
| [`producer_bars.py`](producer_bars.py) | Streams Binance 1-minute OHLCV bars for BTC/USDT and ETH/USDT into the `bars.1m.raw` Kafka topic with deduplication. |
| [`features_minute.py`](features_minute.py) | Consumes raw bar data, aligns the two crypto assets, computes percentage returns, and publishes features to the `features.1m` Kafka topic. |
| [`model_leadlag.py`](model_leadlag.py) | Consumes engineered features, computes beta, spread, and z-scores, outputs trading signals to Kafka, and archives results as daily parquet files. |

## Documentation & Testing Assets

| File | Purpose |
| ---- | ------- |
| [`README_CROSSASSET.md`](README_CROSSASSET.md) | Detailed user guide that covers installation, configuration, and usage of the cross-asset workflow. |
| [`QUICKSTART.md`](QUICKSTART.md) | Short instructions for getting the system running quickly. |
| [`INSTALLATION_NOTES.md`](INSTALLATION_NOTES.md) | Troubleshooting and environment setup tips for external data dependencies. |
| [`BUGFIX_SUMMARY.md`](BUGFIX_SUMMARY.md) & [`BUGFIX_KEYERROR.md`](BUGFIX_KEYERROR.md) | Notes documenting the timestamp alignment fix and the safeguards against empty result sets. |
| [`test_data_fetcher_fix.py`](test_data_fetcher_fix.py) | Demonstrates the timestamp column normalization logic for Yahoo Finance data. |
| [`test_alignment_fix.py`](test_alignment_fix.py) | Walks through the four-step timestamp alignment process and verifies behaviour in overlapping and non-overlapping scenarios. |
| [`test_empty_signals_fix.py`](test_empty_signals_fix.py) | Exercises the lead-lag model on edge cases such as missing timestamps, insufficient window length, and NaNs. |
| [`docker-compose.yml`](docker-compose.yml) | Spins up the Kafka, Zookeeper, and Kafdrop services that support the legacy streaming pipeline. |
| [`requirements.txt`](requirements.txt) | Python dependencies needed for both the batch analysis and streaming components. |
| [`.gitignore`](.gitignore) | Specifies transient files and directories (virtual environments, caches, parquet outputs) that should be excluded from version control. |

This overview is intended to complement the existing documentation by providing a quick reference to the repository's modules and their responsibilities.
